{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries \n",
    "# -----------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.path.abspath(os.path.join('..', 'src'))) \n",
    "\n",
    "\n",
    "# Configuration\n",
    "# -----------------------------------------------------------------------\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Exploration and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo code\n",
    "# Exploratory Data Analysis\n",
    "    # create dataframes DONE\n",
    "    # at dataframe level \n",
    "        # print df name DONE\n",
    "        # visual exploration with .head(), .tail() and sample() DONE\n",
    "        # dataframe overview with .info() DONE\n",
    "        # check duplicates with .duplicated().sum DONE\n",
    "            # create separate dataframe with duplicates if needed DONE\n",
    "        # check null values .isnull().sum() DONE\n",
    "        # explore main statistical metrics for numerical and non numerical columns with .describe() and .describe(include = \"object\")\n",
    "    # at column level\n",
    "        # review columns names with .columns DONE\n",
    "        # review unique values with .unique() DONE\n",
    "        # review value count with .value_counts() DONE\n",
    "        # check null values with .isnull().sum() DONE\n",
    "        # atypical values\n",
    "    # create function/s for the EDA\n",
    "# Transformation\n",
    "# Dataframe union\n",
    "# Save data into a csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“‚ Open data \n",
    "\n",
    "df_customer_flights = pd.read_csv(\"data/customer_flight_activity.csv\")\n",
    "df_customer_loyalty = pd.read_csv(\"data/customer_loyalty_history.csv\")\n",
    "dataframes_dict = {\"df_customer_flightâœˆï¸\" : df_customer_flights, \"df_customer_loyaltyðŸ’ž\" : df_customer_loyalty}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœï¸ Exploratory Data Analysis functions definition\n",
    "\n",
    "def explore_dataframes (df_dict):\n",
    "    \"\"\"\n",
    "    Provides relevant information for a Exploratory Data Analysis.\n",
    "    \n",
    "    This function receives a dict with dataframes and executes a series of \n",
    "    functions aiming to provide relevant information to analize the data.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "    duplicate_dataframes_dict = {}\n",
    "    for name, df in df_dict.items():\n",
    "        df_name = f\"'{name.upper().replace('_',' ')}'\"\n",
    "        print(f\" \\n\\n----------- DATAFRAME NAME: {df_name} -----------\")\n",
    "        print(f\"\\n{df_name} ---> Dataframe INFO:\\n\")\n",
    "        display(df.info())\n",
    "        print(f\"\\n{df_name} ---> FIRST FIVE (5) ROWS:\")\n",
    "        display(df.head())\n",
    "        print(f\"\\n{df_name} ---> LAST FIVE (5) ROWS:\")\n",
    "        display(df.tail())\n",
    "        print(f\"\\n{df_name} ---> SAMPLE (5) ROWS:\")\n",
    "        display(df.sample(5))\n",
    "        print(f\" \\n{df_name} ---> DUPLICATES COUNT IS: {df.duplicated().sum()}, {round((df.duplicated().sum()/df.shape[0]*100),2)}% OVER TOTAL ROWS\\n\")\n",
    "        if df.duplicated().sum() > 0:\n",
    "            duplicates_df_name = name + \"_duplicates\"\n",
    "            duplicates_df = df[df.duplicated(keep=False)]\n",
    "            print(f\"{df_name} ---> DATAFRAME WITH DUPLICATED ROWS (INCLUDING ALL APPEARANCES):\\n\")\n",
    "            display(duplicates_df.head(10))\n",
    "            duplicate_dataframes_dict[duplicates_df_name] = duplicates_df\n",
    "        print(f\"\\n{df_name} --> COUNT OF ROWS WITH ALL NULL VALUES IS: {df.isnull().all(axis=1).sum()}\\n\")\n",
    "        print(f\"\\n{df_name} --> COUNT OF COLUMNS WITH ALL NULL VALUES IS: {df.isnull().all().sum()}\\n\")\n",
    "        print(F\"\\n{df_name} --> STATISTICAL METRICS FOR NUMERICAL COLUMNS:\")\n",
    "        display(df.describe().T)\n",
    "        try:\n",
    "            print(F\"\\n{df_name} --> STATISTICAL METRICS FOR CATEGORICAL COLUMNS:\")\n",
    "            display(df.describe(include=\"object\").T)\n",
    "        except:\n",
    "            print(\"\\nUPS... IT SEEMS LIKE THERE ARE NO COLUMNS WITH CATEGORICATL DATA\")\n",
    "    return duplicate_dataframes_dict\n",
    "\n",
    "def explore_columns (df_dict):\n",
    "    for name, df in df_dict.items():\n",
    "        df_name = f\"'{name.upper().replace('_',' ')}'\"\n",
    "        print(f\" \\n\\n----------- DATAFRAME NAME: {df_name} -----------\")\n",
    "        for index, column in enumerate(df.columns):\n",
    "            print (f\"\\n{index}) Column {column.upper()} (from {(df_name)} dataframe):\")\n",
    "            print (f\"\\n>>> UNIQUE VALUES:\")\n",
    "            print (df[column].unique())\n",
    "            print (f\"\\n>>> VALUES COUNT:\")\n",
    "            print (df[column].value_counts())\n",
    "            print (f\"\\n>>> COUNT OF DUPLICATES IN THE COLUMN:\")\n",
    "            print (df.duplicated(subset=[column]).sum())\n",
    "            print (f\"\\n>>> COUNT OF NULL VALUES IN THE COLUMN:\")\n",
    "            print (df[column].isnull().sum())\n",
    "            if df[column].dtype in ['int64', 'float64']:\n",
    "                print(\"\\nSTATISTICAL DESCRIPTION (NUMERIC):\")\n",
    "                display(df[column].describe())\n",
    "            else:\n",
    "                print(\"\\nSTATISTICAL DESCRIPTION (CATEGORICAL):\")\n",
    "                display(df[column].describe(include='object'))\n",
    "            print (\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â–¶ï¸ Exploratory Data Analysis code execution\n",
    "\n",
    "explore_dataframes(dataframes_dict)\n",
    "\n",
    "explore_columns(dataframes_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœï¸ Data transformation (cleaning and union) functions definition\n",
    "\n",
    "def columns_to_snake_case (df_dict):\n",
    "    for name, df in df_dict.items():\n",
    "        df_name = f\"'{name.upper().replace('_',' ')}'\"\n",
    "        df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "        print(f\"\\n{df_name} ---> Dataframe COLUMNS:\\n\")\n",
    "        print(df.columns)\n",
    "\n",
    "def impute_nulls_as_special_category(df, column_list, category_name):\n",
    "    # Iterate through the list of columns to replace nulls with \"Unknown\"\n",
    "    for column in column_list:\n",
    "        if column in df.columns:\n",
    "            # Replace nulls with the value \"Unknown\" for each column in the list\n",
    "            df[column] = df[column].fillna(category_name)\n",
    "            print (f\"\\nNull values imputed âœ… in column {column}.\")\n",
    "            print (f\"UNIQUE VALUES:\")\n",
    "            print (df[column].unique())\n",
    "        else:\n",
    "            print(f\"âŒ The column '{column}' does not exist in the DataFrame.\")\n",
    "\n",
    "def transform_negative_values(df,column_list):\n",
    "    \"\"\"Transform negative values into its absolute value\"\"\"\n",
    "    for column in column_list:\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].abs()\n",
    "            print (f\"\\nNegative values transformed âœ… in column {column}.\")\n",
    "            print (f\"UNIQUE VALUES:\")\n",
    "            print (df[column].unique())\n",
    "        else:\n",
    "            print(f\"âŒ The column '{column}' does not exist in the DataFrame.\")\n",
    "\n",
    "def impute_nulls_as_median(df, column_list):\n",
    "    # Iterate through the list of columns to replace nulls with median\n",
    "    for column in column_list:\n",
    "        if column in df.columns:\n",
    "            median= df[column].median()\n",
    "            # Replace nulls with the median for each column in the list\n",
    "            df[column] = df[column].fillna(median)\n",
    "            print (f\"\\nNull values imputed âœ… in column {column}.\")\n",
    "            print (f\"UNIQUE VALUES:\")\n",
    "            print (df[column].unique())\n",
    "        else:\n",
    "            print(f\"âŒ The column '{column}' does not exist in the DataFrame.\")\n",
    "\n",
    "def dfs_left_union (df_left, df_right):\n",
    "    print (f\"\\nDataframe on the left's SHAPE:{df_left.shape}\")\n",
    "    print (f\"\\nDataframe on the right's SHAPE:{df_right.shape}\")\n",
    "    df_final = df_customer_flights.merge(df_customer_loyalty, how='left', on='loyalty_number')\n",
    "    print(\"\\nDatafrems were joined succesfully âœ…\")\n",
    "    print (f\"\\nDataframe Final's SHAPE:{df_final.shape}\")\n",
    "    print (f\"\\nDataframe Final's COLUMNS are:{df_final.columns}\")\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â–¶ï¸ Data Transformation code execution\n",
    "\n",
    "# Rename columns to snake case format\n",
    "columns_to_snake_case(dataframes_dict)\n",
    "\n",
    "# Delete records from df customer_flights that are duplicated in all columns, keeping only the first appearance\n",
    "df_customer_flights.drop_duplicates(inplace=True)\n",
    "\n",
    "print(f\"DUPLICATES COUNT IS: {df_customer_flights.duplicated().sum()}\")\n",
    "\n",
    "# In columns \"cancellation_year\" and \"cancelation_month\" replace NaN with \"Not Cancelled\"\n",
    "columns = [\"cancellation_year\",\"cancellation_month\"]\n",
    "category_name = \"Not Cancelled\"\n",
    "impute_nulls_as_special_category(df_customer_loyalty,columns,category_name)\n",
    "\n",
    "# In salary column transform negative value into positive and impute nulls assigning the median value\n",
    "columns = [\"salary\"]\n",
    "transform_negative_values(df_customer_loyalty,columns)\n",
    "impute_nulls_as_median(df_customer_loyalty, columns)\n",
    "\n",
    "# Join dataframes \n",
    "df_final = dfs_left_union(df_customer_flights,df_customer_loyalty)\n",
    "\n",
    "# Save data into a csv\n",
    "df_final.to_csv('data/customer_data_transformed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loyalty number column analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loyalty_number_analysis (df):\n",
    "    \n",
    "    # check the number of duplicates in loyalty_number column\n",
    "    duplicates = df_customer_flights.duplicated(subset=['loyalty_number']).sum()\n",
    "    print(f\"There are {duplicates} duplicates in the column 'loyalty_number', which represent {round((duplicates/df_customer_flights.shape[0]*100),2)}% over the total\")\n",
    "    \n",
    "    df_loyalty_number_duplicates = df_customer_flights[df_customer_flights.duplicated(subset=['loyalty_number'])]\n",
    "    print (f\"\\n>>>DATAFRAME WITH loyalty_number:\")\n",
    "    display(df_loyalty_number_duplicates)\n",
    "    # the information in the dataframe is organized in a way that there is one row per each year and each month within a year. \n",
    "    # Since there is info from 2 years it is expected to have 24 rows per loyalty_number (or less). So, I will only focus the analysis in those that appear more than 24 times\n",
    "    # Calculate the times of appearance for each loyalty_number\n",
    "    loyalty_number_value_counts = df_customer_flights['loyalty_number'].value_counts()\n",
    "    print (f\"\\n>>> VALUES COUNT:\")\n",
    "    print(loyalty_number_value_counts)\n",
    "    # filter the Series to keep only the values that appear more than 24 times\n",
    "    values_to_keep = loyalty_number_value_counts[loyalty_number_value_counts > 24].index\n",
    "    print (f\"\\n>>> VALUES WITH MORE THAN 24 ROWS:\")\n",
    "    print(values_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_customer_flights[df_customer_flights['loyalty_number'].isin(values_to_keep)].sort_values(by=['loyalty_number','year', 'month','total_flights'])\n",
    "filtered_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_loyalty[df_customer_loyalty['loyalty_number'].isin(values_to_keep)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
